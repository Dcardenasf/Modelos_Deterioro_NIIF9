{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf03ebf0",
   "metadata": {},
   "source": [
    "# Modelamiento\n",
    "En este notebook se realizará el entrenamiento y validación de los modelos de riesgo crediticio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196fc38",
   "metadata": {},
   "source": [
    "## Cargue de librerías y parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Agrega la ruta del directorio 'src' al path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Importar los módulos\n",
    "from procesamiento_datos import *\n",
    "from modelado import *\n",
    "from evaluacion import *\n",
    "\n",
    "# Importar bibliotecas adicionales para modelamiento\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70813f",
   "metadata": {},
   "source": [
    "## Cargue de datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos preprocesados\n",
    "df = pd.read_csv('../data/processed/datos_modelado.csv')\n",
    "\n",
    "# Convertir la fecha a datetime\n",
    "df['fecha_cierre'] = pd.to_datetime(df['fecha_cierre'])\n",
    "\n",
    "print(\"Dimensiones del dataset:\", df.shape)\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nDistribución de la variable objetivo:\")\n",
    "print(df['default'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b74f63",
   "metadata": {},
   "source": [
    "## Preparación de datos para modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580865f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar features para el modelo\n",
    "features_numericas = ['dias_mora', 'score_credito', 'promedio_dias_mora', \n",
    "                     'porcentaje_pagos_tiempo', 'monto_promedio_facturas',\n",
    "                     'ratio_pago_promedio', 'volatilidad_dias_mora',\n",
    "                     'facturas_vencidas_actuales', 'saldo_pendiente_total',\n",
    "                     'ipc', 'pib', 'tasa_credito', 'salario_minimo']\n",
    "\n",
    "features_categoricas = ['categoria_riesgo', 'calificacion', 'nombre_linea']\n",
    "\n",
    "# Preparar features numéricas\n",
    "X_num = df[features_numericas].copy()\n",
    "\n",
    "# Normalizar variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "X_num_scaled = pd.DataFrame(X_num_scaled, columns=X_num.columns)\n",
    "\n",
    "# Preparar features categóricas (one-hot encoding)\n",
    "X_cat = pd.get_dummies(df[features_categoricas])\n",
    "\n",
    "# Combinar features numéricas y categóricas\n",
    "X = pd.concat([X_num_scaled, X_cat], axis=1)\n",
    "y = df['default']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dimensiones de los conjuntos de datos:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "# Verificar proporción de clases en los conjuntos\n",
    "print(\"\\nDistribución de clases:\")\n",
    "print(\"Train:\", pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Test:\", pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecede9",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo de Regresión Logística\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar XGBoost\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Guardar los modelos entrenados\n",
    "modelos = {\n",
    "    'logistic_regression': lr_model,\n",
    "    'random_forest': rf_model,\n",
    "    'xgboost': xgb_model\n",
    "}\n",
    "\n",
    "# Crear directorio para modelos si no existe\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar modelos\n",
    "for nombre, modelo in modelos.items():\n",
    "    joblib.dump(modelo, f'../models/{nombre}_model.pkl')\n",
    "    print(f\"Modelo {nombre} guardado en ../models/{nombre}_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a399ca",
   "metadata": {},
   "source": [
    "## Evaluación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440728e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar los modelos\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"\\nEvaluación del modelo {nombre}:\")\n",
    "    metricas = evaluar_modelo(modelo, X_test, y_test, nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af366a6f",
   "metadata": {},
   "source": [
    "## Análisis de importancia de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5818a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar importancia de variables\n",
    "analizar_importancia_variables({\n",
    "    'random_forest': rf_model,\n",
    "    'xgboost': xgb_model\n",
    "}, X_test, feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b73331",
   "metadata": {},
   "source": [
    "# Modelamiento de Riesgo de Default\n",
    "\n",
    "Este notebook se enfoca en el desarrollo y entrenamiento de modelos predictivos para identificar el riesgo de default en clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d08d9",
   "metadata": {},
   "source": [
    "## Carga y Preparación de Datos\n",
    "\n",
    "Cargaremos el dataset procesado y realizaremos la preparación necesaria para el modelamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bd2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset:\n",
      "X: (15916, 844)\n",
      "y: (15916,)\n",
      "\n",
      "Distribución de la variable objetivo:\n",
      "default\n",
      "0    0.876791\n",
      "1    0.123209\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos procesados\n",
    "df = pd.read_csv('../data/processed/datos_modelado.csv')\n",
    "\n",
    "# Seleccionar features para el modelo\n",
    "features_numericas = [\n",
    "    'score_credito', 'promedio_dias_mora', 'porcentaje_pagos_tiempo',\n",
    "    'monto_promedio_facturas', 'ratio_pago_promedio', 'volatilidad_dias_mora',\n",
    "    'maximo_dias_mora', 'facturas_vencidas_actuales', 'saldo_pendiente_total',\n",
    "    'ipc', 'pib', 'tasa_credito', 'salario_minimo',\n",
    "    'var_ipc', 'var_pib', 'var_tasa_credito', 'var_salario_minimo'\n",
    "]\n",
    "\n",
    "features_categoricas = ['categoria_riesgo', 'id_negocio']\n",
    "\n",
    "# Preparar X e y\n",
    "X = df[features_numericas + features_categoricas].copy()\n",
    "y = df['default']\n",
    "\n",
    "# Codificar variables categóricas\n",
    "X = pd.get_dummies(X, columns=features_categoricas)\n",
    "\n",
    "print(\"Dimensiones del dataset:\")\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "\n",
    "# Verificar balance de clases\n",
    "print(\"\\nDistribución de la variable objetivo:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22449740",
   "metadata": {},
   "source": [
    "## Preprocesamiento y División de Datos\n",
    "\n",
    "Escalaremos las variables numéricas y dividiremos los datos en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c3d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los conjuntos de datos:\n",
      "X_train: (12732, 844)\n",
      "X_test: (3184, 844)\n",
      "y_train: (12732,)\n",
      "y_test: (3184,)\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Escalar variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Obtener columnas numéricas (excluyendo las dummies)\n",
    "columnas_numericas = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Aplicar escalado\n",
    "X_train_scaled[columnas_numericas] = scaler.fit_transform(X_train[columnas_numericas])\n",
    "X_test_scaled[columnas_numericas] = scaler.transform(X_test[columnas_numericas])\n",
    "\n",
    "# Guardar los datos procesados\n",
    "X_train_scaled.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "print(\"Dimensiones de los conjuntos de datos:\")\n",
    "print(f\"X_train: {X_train_scaled.shape}\")\n",
    "print(f\"X_test: {X_test_scaled.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0509d",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "\n",
    "Antes de escalar los datos, nos aseguraremos de que no haya valores faltantes o infinitos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para verificar datos\n",
    "def verificar_datos(X, nombre=\"Dataset\"):\n",
    "    nas = X.isna().sum()\n",
    "    nas_total = nas.sum()\n",
    "    inf_count = np.isinf(X.select_dtypes(include=np.number)).sum().sum()\n",
    "    \n",
    "    print(f\"\\nVerificación de {nombre}:\")\n",
    "    if nas_total > 0:\n",
    "        print(\"\\nColumnas con valores faltantes:\")\n",
    "        print(nas[nas > 0])\n",
    "    print(f\"\\nTotal de valores faltantes: {nas_total}\")\n",
    "    print(f\"Valores infinitos: {inf_count}\")\n",
    "\n",
    "# Función para limpiar datos\n",
    "def limpiar_datos(X):\n",
    "    X_clean = X.copy()\n",
    "    # Reemplazar infinitos con NaN\n",
    "    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)\n",
    "    # Para cada columna, imputar con la mediana si hay valores faltantes\n",
    "    for col in X_clean.columns:\n",
    "        if X_clean[col].isna().any():\n",
    "            mediana = X_clean[col].median()\n",
    "            X_clean[col] = X_clean[col].fillna(mediana)\n",
    "    return X_clean\n",
    "\n",
    "# Verificar estado inicial\n",
    "print(\"Estado inicial de los datos:\")\n",
    "verificar_datos(X_train, \"X_train\")\n",
    "verificar_datos(X_test, \"X_test\")\n",
    "\n",
    "# Limpiar datos\n",
    "X_train = limpiar_datos(X_train)\n",
    "X_test = limpiar_datos(X_test)\n",
    "\n",
    "# Verificar después de la limpieza\n",
    "print(\"\\nDespués de la limpieza:\")\n",
    "verificar_datos(X_train, \"X_train\")\n",
    "verificar_datos(X_test, \"X_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff6a61",
   "metadata": {},
   "source": [
    "## Escalado de Datos\n",
    "\n",
    "Ahora que los datos están limpios, procedemos con el escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f06174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Obtener columnas numéricas (excluyendo las dummies)\n",
    "columnas_numericas = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Aplicar escalado\n",
    "X_train_scaled[columnas_numericas] = scaler.fit_transform(X_train[columnas_numericas])\n",
    "X_test_scaled[columnas_numericas] = scaler.transform(X_test[columnas_numericas])\n",
    "\n",
    "# Verificar que no haya valores faltantes o infinitos después del escalado\n",
    "print(\"Verificación final después del escalado:\")\n",
    "verificar_datos(X_train_scaled, \"X_train_scaled\")\n",
    "verificar_datos(X_test_scaled, \"X_test_scaled\")\n",
    "\n",
    "# Guardar los datos procesados y limpios\n",
    "X_train_scaled.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "print(\"\\nDimensiones finales de los conjuntos de datos:\")\n",
    "print(f\"X_train: {X_train_scaled.shape}\")\n",
    "print(f\"X_test: {X_test_scaled.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0446",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "\n",
    "Verificaremos y limpiaremos valores faltantes o infinitos en nuestros datos antes del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbad3fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de X_train_scaled:\n",
      "Valores faltantes: 10788\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test_scaled:\n",
      "Valores faltantes: 2583\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_train_scaled (después de limpieza):\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test_scaled (después de limpieza):\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_train_scaled (después de limpieza):\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test_scaled (después de limpieza):\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores faltantes o infinitos\n",
    "def verificar_datos(X, nombre=\"Dataset\"):\n",
    "    print(f\"\\nVerificación de {nombre}:\")\n",
    "    print(f\"Valores faltantes: {X.isna().sum().sum()}\")\n",
    "    print(f\"Valores infinitos: {np.isinf(X.select_dtypes(include=np.number)).sum().sum()}\")\n",
    "    \n",
    "verificar_datos(X_train_scaled, \"X_train_scaled\")\n",
    "verificar_datos(X_test_scaled, \"X_test_scaled\")\n",
    "\n",
    "# Reemplazar valores infinitos con NaN\n",
    "X_train_scaled = X_train_scaled.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_scaled = X_test_scaled.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Imputar valores faltantes con la mediana\n",
    "for col in X_train_scaled.columns:\n",
    "    if X_train_scaled[col].isna().any():\n",
    "        mediana = X_train_scaled[col].median()\n",
    "        X_train_scaled[col] = X_train_scaled[col].fillna(mediana)\n",
    "        X_test_scaled[col] = X_test_scaled[col].fillna(mediana)\n",
    "\n",
    "# Verificar después de la limpieza\n",
    "verificar_datos(X_train_scaled, \"X_train_scaled (después de limpieza)\")\n",
    "verificar_datos(X_test_scaled, \"X_test_scaled (después de limpieza)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169f5b3",
   "metadata": {},
   "source": [
    "## Entrenamiento de Modelos (con datos limpios)\n",
    "\n",
    "Ahora entrenaremos los modelos con los datos limpios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fbef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando logistic...\n",
      "logistic:\n",
      "CV ROC-AUC: 0.941 (+/- 0.007)\n",
      "Test ROC-AUC: 0.946\n",
      "\n",
      "Entrenando random_forest...\n",
      "logistic:\n",
      "CV ROC-AUC: 0.941 (+/- 0.007)\n",
      "Test ROC-AUC: 0.946\n",
      "\n",
      "Entrenando random_forest...\n",
      "random_forest:\n",
      "CV ROC-AUC: 0.980 (+/- 0.006)\n",
      "Test ROC-AUC: 0.987\n",
      "\n",
      "Entrenando xgboost...\n",
      "random_forest:\n",
      "CV ROC-AUC: 0.980 (+/- 0.006)\n",
      "Test ROC-AUC: 0.987\n",
      "\n",
      "Entrenando xgboost...\n",
      "xgboost:\n",
      "CV ROC-AUC: 0.972 (+/- 0.003)\n",
      "Test ROC-AUC: 0.975\n",
      "xgboost:\n",
      "CV ROC-AUC: 0.972 (+/- 0.003)\n",
      "Test ROC-AUC: 0.975\n",
      "\n",
      "Modelos guardados en el directorio '../models'\n",
      "\n",
      "Modelos guardados en el directorio '../models'\n"
     ]
    }
   ],
   "source": [
    "# Importar joblib para guardar los modelos\n",
    "import joblib\n",
    "\n",
    "# Crear y entrenar los modelos\n",
    "models = {\n",
    "    'logistic': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'xgboost': xgb.XGBClassifier(scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "                                random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Entrenamiento\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Validación cruzada\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results[name] = {\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "    print(f\"Test ROC-AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Crear directorio para modelos si no existe\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar los modelos\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, f'../models/{name}_model.pkl')\n",
    "\n",
    "print(\"\\nModelos guardados en el directorio '../models'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
