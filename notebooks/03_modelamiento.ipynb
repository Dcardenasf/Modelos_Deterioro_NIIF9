{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf03ebf0",
   "metadata": {},
   "source": [
    "# Modelamiento\n",
    "En este notebook se realizará el entrenamiento y validación de los modelos de riesgo crediticio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196fc38",
   "metadata": {},
   "source": [
    "## Cargue de librerías y parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b03e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de procesamiento de datos cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Agrega la ruta del directorio 'src' al path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Importar los módulos\n",
    "from procesamiento_datos import *\n",
    "from modelado import *\n",
    "from evaluacion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84db24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "datos_raw = \"..//data/raw/\"\n",
    "datos_processed = \"..//data/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70813f",
   "metadata": {},
   "source": [
    "## Cargue de datos preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f80596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (2697, 39)\n",
      "\n",
      "Columnas disponibles:\n",
      "['NIT', 'Total_facturado', 'Total_por_pagar', 'Total_pagado', 'Cantidad_facturas', 'Dias_mora_promedio', 'Porcentaje_pagos_tiempo', 'Promedio_ratio_pago', 'Desv_dias_demora', 'Maximo_dias_demora', 'Facturas_vencidas', 'Factura_promedio', 'Porcentaje_facturas_vencidas', 'score_credito', 'categoria_riesgo', 'default', 'PERIODO', 'FECHA_CORTE', 'ACTIVOS_CORRIENTES', 'GANANCIAS', 'PATRIMONIO', 'TOTAL_ACTIVOS', 'ACTIVOS_NO_CORRIENTES', 'PASIVOS_NO_CORRIENTES', 'TOTAL_PASIVOS', 'VAR_ACTIVOS_CORRIENTES', 'VAR_GANANCIAS', 'VAR_PATRIMONIO', 'VAR_ACTIVOS', 'VAR_Activos_NO_CORRIENTES', 'VAR_PASIVOS_NO_CORRIENTES', 'VAR_TOTAL_PASIVOS', 'PASIVOS_CORRIENTES', 'VAR_PASIVOS_CORRIENTES', 'UTILIDAD_NETA', 'LIQUIDEZ_CORRIENTE', 'CAPITAL_TRABAJO', 'ENDEUDAMIENTO', 'ROA']\n",
      "\n",
      "Distribución de la variable objetivo:\n",
      "default\n",
      "0    95.068595\n",
      "1     4.931405\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos preprocesados\n",
    "df = cargar_datos(datos_processed+'datos_modelado.csv')\n",
    "\n",
    "print(\"Dimensiones del dataset:\", df.shape)\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nDistribución de la variable objetivo:\")\n",
    "print(df['default'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0509d",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "\n",
    "Antes de escalar los datos, nos aseguraremos de que no haya valores faltantes o infinitos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b73331",
   "metadata": {},
   "source": [
    "# Modelamiento de Riesgo de Default\n",
    "\n",
    "Este notebook se enfoca en el desarrollo y entrenamiento de modelos predictivos para identificar el riesgo de default en clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d08d9",
   "metadata": {},
   "source": [
    "## Carga y Preparación de Datos\n",
    "\n",
    "Cargaremos el dataset procesado y realizaremos la preparación necesaria para el modelamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0671f686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado inicial de los datos:\n",
      "\n",
      "Verificación de X_train:\n",
      "\n",
      "Total de valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test:\n",
      "\n",
      "Total de valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Después de la limpieza:\n",
      "\n",
      "Verificación de X_train:\n",
      "\n",
      "Total de valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test:\n",
      "\n",
      "Total de valores faltantes: 0\n",
      "Valores infinitos: 0\n"
     ]
    }
   ],
   "source": [
    "# Función para verificar datos\n",
    "def verificar_datos(X, nombre=\"Dataset\"):\n",
    "    nas = X.isna().sum()\n",
    "    nas_total = nas.sum()\n",
    "    inf_count = np.isinf(X.select_dtypes(include=np.number)).sum().sum()\n",
    "    \n",
    "    print(f\"\\nVerificación de {nombre}:\")\n",
    "    if nas_total > 0:\n",
    "        print(\"\\nColumnas con valores faltantes:\")\n",
    "        print(nas[nas > 0])\n",
    "    print(f\"\\nTotal de valores faltantes: {nas_total}\")\n",
    "    print(f\"Valores infinitos: {inf_count}\")\n",
    "\n",
    "# Función para limpiar datos\n",
    "def limpiar_datos(X):\n",
    "    X_clean = X.copy()\n",
    "    # Reemplazar infinitos con NaN\n",
    "    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)\n",
    "    # Para cada columna, imputar con la mediana si hay valores faltantes\n",
    "    for col in X_clean.columns:\n",
    "        if X_clean[col].isna().any():\n",
    "            mediana = X_clean[col].median()\n",
    "            X_clean[col] = X_clean[col].fillna(mediana)\n",
    "    return X_clean\n",
    "\n",
    "# Verificar estado inicial\n",
    "print(\"Estado inicial de los datos:\")\n",
    "verificar_datos(X_train, \"X_train\")\n",
    "verificar_datos(X_test, \"X_test\")\n",
    "\n",
    "# Limpiar datos\n",
    "X_train = limpiar_datos(X_train)\n",
    "X_test = limpiar_datos(X_test)\n",
    "\n",
    "# Verificar después de la limpieza\n",
    "print(\"\\nDespués de la limpieza:\")\n",
    "verificar_datos(X_train, \"X_train\")\n",
    "verificar_datos(X_test, \"X_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff6a61",
   "metadata": {},
   "source": [
    "## Escalado de Datos\n",
    "\n",
    "Ahora que los datos están limpios, procedemos con el escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f06174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificación final después del escalado:\n",
      "\n",
      "Verificación de X_train_scaled:\n",
      "\n",
      "Total de valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test_scaled:\n",
      "\n",
      "Total de valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Dimensiones finales de los conjuntos de datos:\n",
      "X_train: (2157, 2716)\n",
      "X_test: (540, 2716)\n",
      "y_train: (2157,)\n",
      "y_test: (540,)\n"
     ]
    }
   ],
   "source": [
    "# Escalar variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Obtener columnas numéricas (excluyendo las dummies)\n",
    "columnas_numericas = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Aplicar escalado\n",
    "X_train_scaled[columnas_numericas] = scaler.fit_transform(X_train[columnas_numericas])\n",
    "X_test_scaled[columnas_numericas] = scaler.transform(X_test[columnas_numericas])\n",
    "\n",
    "# Verificar que no haya valores faltantes o infinitos después del escalado\n",
    "print(\"Verificación final después del escalado:\")\n",
    "verificar_datos(X_train_scaled, \"X_train_scaled\")\n",
    "verificar_datos(X_test_scaled, \"X_test_scaled\")\n",
    "\n",
    "# Guardar los datos procesados y limpios\n",
    "X_train_scaled.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "print(\"\\nDimensiones finales de los conjuntos de datos:\")\n",
    "print(f\"X_train: {X_train_scaled.shape}\")\n",
    "print(f\"X_test: {X_test_scaled.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0446",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "\n",
    "Verificaremos y limpiaremos valores faltantes o infinitos en nuestros datos antes del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbad3fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificación de X_train_scaled:\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test_scaled:\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_train_scaled (después de limpieza):\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n",
      "\n",
      "Verificación de X_test_scaled (después de limpieza):\n",
      "Valores faltantes: 0\n",
      "Valores infinitos: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar valores faltantes o infinitos\n",
    "def verificar_datos(X, nombre=\"Dataset\"):\n",
    "    print(f\"\\nVerificación de {nombre}:\")\n",
    "    print(f\"Valores faltantes: {X.isna().sum().sum()}\")\n",
    "    print(f\"Valores infinitos: {np.isinf(X.select_dtypes(include=np.number)).sum().sum()}\")\n",
    "    \n",
    "verificar_datos(X_train_scaled, \"X_train_scaled\")\n",
    "verificar_datos(X_test_scaled, \"X_test_scaled\")\n",
    "\n",
    "# Reemplazar valores infinitos con NaN\n",
    "X_train_scaled = X_train_scaled.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_scaled = X_test_scaled.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Imputar valores faltantes con la mediana\n",
    "for col in X_train_scaled.columns:\n",
    "    if X_train_scaled[col].isna().any():\n",
    "        mediana = X_train_scaled[col].median()\n",
    "        X_train_scaled[col] = X_train_scaled[col].fillna(mediana)\n",
    "        X_test_scaled[col] = X_test_scaled[col].fillna(mediana)\n",
    "\n",
    "# Verificar después de la limpieza\n",
    "verificar_datos(X_train_scaled, \"X_train_scaled (después de limpieza)\")\n",
    "verificar_datos(X_test_scaled, \"X_test_scaled (después de limpieza)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169f5b3",
   "metadata": {},
   "source": [
    "## Entrenamiento de Modelos (con datos limpios)\n",
    "\n",
    "Ahora entrenaremos los modelos con los datos limpios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fbef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando logistic...\n",
      "logistic:\n",
      "CV ROC-AUC: 1.000 (+/- 0.000)\n",
      "Test ROC-AUC: 1.000\n",
      "\n",
      "Entrenando random_forest...\n",
      "random_forest:\n",
      "CV ROC-AUC: 1.000 (+/- 0.000)\n",
      "Test ROC-AUC: 1.000\n",
      "\n",
      "Entrenando xgboost...\n",
      "xgboost:\n",
      "CV ROC-AUC: 1.000 (+/- 0.000)\n",
      "Test ROC-AUC: 1.000\n",
      "\n",
      "Modelos guardados en el directorio '../models'\n"
     ]
    }
   ],
   "source": [
    "# Importar joblib para guardar los modelos\n",
    "import joblib\n",
    "\n",
    "# Crear y entrenar los modelos\n",
    "models = {\n",
    "    'logistic': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'xgboost': xgb.XGBClassifier(scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "                                random_state=42)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Entrenamiento\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Validación cruzada\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results[name] = {\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"CV ROC-AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "    print(f\"Test ROC-AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# Crear directorio para modelos si no existe\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar los modelos\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, f'../models/{name}_model.pkl')\n",
    "\n",
    "print(\"\\nModelos guardados en el directorio '../models'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
